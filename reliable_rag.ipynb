{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db7f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI, OpenAIError\n",
    "\n",
    "load_dotenv()  # .env 파일에 있는 환경변수 로드\n",
    "\n",
    "aoi_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoi_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoi_gen_model = os.getenv(\"AZURE_GENERATION_MODEL\")\n",
    "aoi_version = os.getenv(\"AZURE_GENERATION_MODEL_VERSION\")\n",
    "client = AzureOpenAI(azure_endpoint=aoi_endpoint,api_key=aoi_api_key, api_version=aoi_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f5d0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "# Set embeddings\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "                                   deployment=\"text-embedding-ada-002\",\n",
    "                                   openai_api_key=aoi_api_key,\n",
    "                                   azure_endpoint=aoi_endpoint,\n",
    "                                   api_version=aoi_version\n",
    "                                  )\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "991bc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=aoi_endpoint,\n",
    "    api_key=aoi_api_key, \n",
    "    api_version=aoi_version,\n",
    "    deployment_name=aoi_gen_model\n",
    ")\n",
    "structured_llm_grader = llm.with_structured_output(\n",
    "    GradeDocuments,\n",
    "    method=\"function_calling\"\n",
    ")\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    \"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2adbd092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say, scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well.In many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows. While managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans! Emerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their GitHub repo and perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does. Like the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning \n",
      " --------------------------------------------------\n",
      "yes \n",
      "\n",
      "Agentic Design Patterns Part 4: Planning✨ Updated course! Enroll in LLMs as Operating Systems: Agent MemoryExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 4, Planning Large language models can drive powerful agents to execute complex tasks if you ask them to plan the steps before they act.LettersTechnical InsightsPublishedApr 10, 2024Reading time3 min readShareDear friends,Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report. Many people had a “ChatGPT moment” shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar “AI Agentic moment,” I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools. I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool — which I had forgotten I’d given it — and completed the task using Wikipedia instead of web search. This was an AI Agentic moment of surprise for me. I think many people who haven’t experienced such a moment yet will do so in the coming months. It’s a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!Many tasks can’t be done in a single step \n",
      " --------------------------------------------------\n",
      "yes \n",
      "\n",
      "Agentic Design Patterns Part 5, Multi-Agent Collaboration✨ Updated course! Enroll in LLMs as Operating Systems: Agent MemoryExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 5, Multi-Agent Collaboration Prompting an LLM to play different roles for different parts of a complex task summons a team of AI agents that can do the job more effectively.LettersTechnical InsightsPublishedApr 17, 2024Reading time3 min readShareDear friends,Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..” It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons:It works! Many teams are getting good results with this method, and there’s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent. Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role’s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, \n",
      " --------------------------------------------------\n",
      "yes \n",
      "\n",
      "Agentic Design Patterns Part 2: Reflection✨ Updated course! Enroll in LLMs as Operating Systems: Agent MemoryExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 2, Reflection Large language models can become more effective agents by reflecting on their own behavior.LettersTechnical InsightsPublishedMar 27, 2024Reading time2 min readShareDear friends,Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool Use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains. You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection. Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:Here’s code intended for task X: [previously generated code]    Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and the constructive feedback and (ii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including \n",
      " --------------------------------------------------\n",
      "yes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_to_use = []\n",
    "for doc in docs:\n",
    "    print(doc.page_content, '\\n', '-'*50)\n",
    "    # Invoke the grader with a dictionary input, expecting a structured GradeDocuments output\n",
    "    response = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    binary_answer = response.binary_score.lower()\n",
    "\n",
    "    print(binary_answer,'\\n')\n",
    "    if binary_answer == 'yes':\n",
    "        docs_to_use.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6eaa853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different kinds of agentic design patterns mentioned in the retrieved documents are:\n",
      "\n",
      "1. **Reflection**: Involves the agent reflecting on its own output and providing critical feedback to improve its response with each iteration.\n",
      "2. **Tool Use**: This pattern was mentioned but not described in the retrieved documents.\n",
      "3. **Planning**: An agent autonomously plans the sequence of steps to execute in order to accomplish a larger task.\n",
      "4. **Multi-Agent Collaboration**: Involves using multiple agents, each with distinct roles, to collaboratively handle different subtasks of a complex task.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=aoi_endpoint,\n",
    "    api_key=aoi_api_key, \n",
    "    api_version=aoi_version,\n",
    "    deployment_name=aoi_gen_model\n",
    ")\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a57fd0",
   "metadata": {},
   "source": [
    "### 주요 학습 내용: 검증 및 개선 단계를 통한 RAG 파이프라인 강화\n",
    "\n",
    "이번 스크립트 실습과 논의를 통해, 기본적인 RAG (Retrieval-Augmented Generation) 파이프라인에 **검증(Validation) 및 개선(Refinement) 단계를 추가**하여, 검색된 정보의 **정확성(Accuracy)과 관련성(Relevance)을 보장**하고 전체 프로세스를 강화하는 방법을 배웠습니다. 이 과정에서 Pydantic과 LangChain의 `with_structured_output` 기능을 효과적으로 활용하여 LLM 응답의 신뢰성을 높이는 것이 핵심이었습니다.\n",
    "\n",
    "**핵심 구현 방식 및 효과:**\n",
    "\n",
    "* **1. 문제 인식 및 목표 설정:**\n",
    "    * 단순 RAG는 관련성이 낮거나 부정확한 정보를 검색하여 최종 답변의 품질을 저하시키거나, LLM이 환각(Hallucination)을 일으킬 수 있습니다.\n",
    "    * **목표:** 검색된 정보의 품질을 확인하고 개선하여 RAG 파이프라인의 신뢰도를 높입니다.\n",
    "\n",
    "* **2. 검증/개선 단계 구현 (문서 관련성 평가):**\n",
    "    * **Pydantic 모델 정의 (`GradeDocuments`):**\n",
    "        * LLM이 수행할 작업(관련성 판단)의 결과(예: 'yes'/'no')를 **명확하고 구조화된 형식**으로 정의했습니다.\n",
    "        * `Field`의 `description`을 사용하여 각 필드의 의미와 기대값을 LLM에게 **힌트로 제공**했습니다. (단, 시스템 프롬프트의 직접적인 지시와 충돌 시 프롬프트가 우선될 수 있음을 인지했습니다.)\n",
    "        * 이는 LLM의 응답에서 모호함을 제거하고 **일관된 형식**을 갖도록 유도합니다.\n",
    "    * **`with_structured_output` 활용:**\n",
    "        * LLM(`AzureChatOpenAI`)이 **반드시 정의된 Pydantic 구조에 따라 응답하도록 강제**했습니다 (`method=\"function_calling\"` 사용).\n",
    "        * 이를 통해 LLM의 출력을 프로그램 코드에서 **신뢰성 있게 파싱하고 활용**할 수 있게 되었습니다 (`response.binary_score` 와 같이 직접 접근 가능).\n",
    "    * **자동 필터링 로직:**\n",
    "        * 구조화되고 검증된 LLM의 판단 결과('yes'/'no')를 바탕으로, 관련 없는 문서를 **자동으로 필터링**하는 로직을 손쉽게 구현했습니다.\n",
    "\n",
    "* **3. 파이프라인 강화 효과:**\n",
    "    * **향상된 관련성 및 정확성:** 최종 답변 생성 LLM에게 **더 관련성 높은 정보만을 컨텍스트로 제공**함으로써, 답변의 정확성과 주제 관련성을 높였습니다.\n",
    "    * **환각 제어 (간접적):** LLM의 출력을 특정 구조로 제한하고, 검증된 컨텍스트를 제공함으로써 LLM이 자유롭게 정보를 지어내는 **환각 현상을 억제**하는 데 도움을 줄 수 있습니다.\n",
    "    * **신뢰성 및 예측 가능성 증대:** 전체 RAG 파이프라인의 작동 방식을 더 예측 가능하게 만들고, 최종 결과물에 대한 **신뢰도**를 향상시켰습니다.\n",
    "\n",
    "결론적으로, Pydantic을 사용한 명확한 데이터 스키마 정의와 `with_structured_output`을 통한 LLM 출력 제어 및 검증은, LLM 기반 애플리케이션에서 **정보의 품질을 관리하고 개선**하여 **더욱 정확하고 신뢰할 수 있는 시스템**을 구축하기 위한 핵심적인 기법임을 배웠습니다. 이는 단순 RAG를 넘어서는 중요한 개선 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9a7ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
